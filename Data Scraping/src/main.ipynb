{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d1664e",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid #8E7B6B; margin-top: 10px;\">\n",
    "\n",
    "<br>\n",
    "<h1 style=\"font-family:verdana; font-size:36px; font-weight:bold\"> <center>~ Notebook Scraper Paris 2024 Olympic ~</center> </h1>\n",
    "<p style = \"font-size:16px; font-family:verdana\"><center>Oleh: Izhar Alif Akbar / 18223129 </center><p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"border: 2px solid #8E7B6B; margin-top: 10px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2585a5",
   "metadata": {},
   "source": [
    "# ~ Notebook Contents ~\n",
    "\n",
    "1. [**Medal Scraper**](#1)\n",
    "\n",
    "2. [**Sport Scraper**](#2)\n",
    "\n",
    "3. [**Athlete Scraper**](#3)\n",
    "\n",
    "4. [**Data Pre-processing**](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b042950",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid #8E7B6B; margin-top: 10px;\">\n",
    "\n",
    "# Initialization <a name=\"initialization\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5bbcf4",
   "metadata": {},
   "source": [
    "> Note: \"Run All\" pada notebook ini akan memakan waktu yang cukup lama yaitu sekitar 10 jam karena proses scraping. (10k+ data expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617d5082",
   "metadata": {},
   "source": [
    "<hr style=\"border: 2px solid #8E7B6B; margin-top: 10px;\">\n",
    "\n",
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8079714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import requests\n",
    "import re\n",
    "import unicodedata\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Tag\n",
    "from tqdm import tqdm\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e8391",
   "metadata": {},
   "source": [
    "## 1. Medal Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a3ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13:23:44 [INFO] Scraper diinisialisasi dengan User-Agent: Izhar Alif Akbar/18223129@std.stei.itb.ac.id\n",
      "13:23:44 [INFO] Mengakses URL: https://www.olympics.com/en/olympic-games/paris-2024/medals\n",
      "13:23:45 [INFO] Halaman berhasil diakses. Mem-parsing HTML...\n",
      "13:23:45 [INFO] Ditemukan 92 negara di dalam tabel.\n",
      "Scraping Medals: 100%|██████████| 92/92 [00:01<00:00, 59.12it/s] \n",
      "13:23:46 [INFO] Scraping semua negara selesai.\n",
      "13:23:46 [INFO] Menyimpan 92 data ke file c:\\Izhar\\Lab Basdat\\Seleksi-2025-Tugas-1\\Data Scraping\\data\\raw_medal_test.json...\n",
      "13:23:46 [INFO] Penyimpanan data selesai.\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "class MedalScraper:\n",
    "    \"\"\"\n",
    "    Scraper untuk mengambil data tabel medali dari situs Olimpiade.\n",
    "    \"\"\"\n",
    "    URL = \"https://www.olympics.com/en/olympic-games/paris-2024/medals\"\n",
    "    OUTPUT_FILENAME = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw_medals.json\")\n",
    "    SCRAPPER_IDENTITY = 'Izhar Alif Akbar/18223129@std.stei.itb.ac.id'\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Inisialisasi scraper.\"\"\"\n",
    "        self.scraped_data = []\n",
    "        self.headers = {\n",
    "            'User-Agent': f'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        logging.info(f\"Scraper diinisialisasi dengan User-Agent: {self.SCRAPPER_IDENTITY}\")\n",
    "\n",
    "    def _get_soup(self) -> BeautifulSoup | None:\n",
    "        \"\"\"Mengambil konten halaman dan mengembalikannya sebagai objek BeautifulSoup.\"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Mengakses URL: {self.URL}\")\n",
    "            response = requests.get(self.URL, headers=self.headers, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            logging.info(\"Halaman berhasil diakses. Mem-parsing HTML...\")\n",
    "            return BeautifulSoup(response.content, 'lxml')\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Gagal mengambil halaman: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_text_from_selector(self, parent: Tag, selector: str, default: str = \"0\") -> str:\n",
    "        \"\"\"Helper untuk mendapatkan teks dari elemen dengan aman.\"\"\"\n",
    "        element = parent.select_one(selector)\n",
    "        return element.get_text(strip=True) if element else default\n",
    "\n",
    "    def _parse_row(self, row_tag: Tag, table_soup: BeautifulSoup) -> dict:\n",
    "        \"\"\"Mengurai satu baris data negara dari tabel.\"\"\"\n",
    "        row_id = row_tag.get('data-row-id', '')\n",
    "        row_number = row_id.split('-')[-1]\n",
    "\n",
    "        # Cari elemen nama negara yang berelasi dengan row_id\n",
    "        country_name_tag = table_soup.find('div', class_='sc-26c0a561-4', attrs={'data-row-id': row_id})\n",
    "        country_name = self._get_text_from_selector(country_name_tag, 'span.sc-26c0a561-6', \"N/A\")\n",
    "\n",
    "        # Cari data medali menggunakan row_number\n",
    "        gold = self._get_text_from_selector(table_soup, f'div[data-medal-id=\"gold-medals-row-{row_number}\"]')\n",
    "        silver = self._get_text_from_selector(table_soup, f'div[data-medal-id=\"silver-medals-row-{row_number}\"]')\n",
    "        bronze = self._get_text_from_selector(table_soup, f'div[data-medal-id=\"bronze-medals-row-{row_number}\"]')\n",
    "        total = self._get_text_from_selector(table_soup, f'div[data-medal-id=\"total-medals-row-{row_number}\"]')\n",
    "\n",
    "        return {\n",
    "            \"country\": country_name,\n",
    "            \"gold\": int(gold) if gold.isdigit() else 0,\n",
    "            \"silver\": int(silver) if silver.isdigit() else 0,\n",
    "            \"bronze\": int(bronze) if bronze.isdigit() else 0,\n",
    "            \"total\": int(total) if total.isdigit() else 0\n",
    "        }\n",
    "\n",
    "    def scrape(self):\n",
    "        \"\"\"Menjalankan proses scraping utama.\"\"\"\n",
    "        soup = self._get_soup()\n",
    "        if not soup:\n",
    "            return\n",
    "\n",
    "        table_div = soup.find(\"div\", {\"data-cy\": \"table-content\"})\n",
    "        if not table_div:\n",
    "            logging.error(\"Container tabel utama ('table-content') tidak ditemukan.\")\n",
    "            return\n",
    "\n",
    "        # Deteksi baris negara\n",
    "        country_rows = table_div.find_all('div', class_='sc-26c0a561-2')\n",
    "        logging.info(f\"Ditemukan {len(country_rows)} negara di dalam tabel.\")\n",
    "\n",
    "        # TQDM (progress bar)\n",
    "        for row in tqdm(country_rows, desc=\"Scraping Medals\"):\n",
    "            country_data = self._parse_row(row, table_div)\n",
    "            if country_data[\"country\"] != \"N/A\":\n",
    "                self.scraped_data.append(country_data)\n",
    "\n",
    "        logging.info(\"Scraping semua negara selesai.\")\n",
    "\n",
    "    def save_to_json(self):\n",
    "        \"\"\"Menyimpan data yang telah di-scrape ke file JSON.\"\"\"\n",
    "        if not self.scraped_data:\n",
    "            logging.warning(\"Tidak ada data untuk disimpan.\")\n",
    "            return\n",
    "            \n",
    "        logging.info(f\"Menyimpan {len(self.scraped_data)} data ke file {self.OUTPUT_FILENAME}...\")\n",
    "        with open(self.OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.scraped_data, f, ensure_ascii=False, indent=4)\n",
    "        logging.info(\"Penyimpanan data selesai.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = MedalScraper()\n",
    "    scraper.scrape()\n",
    "    scraper.save_to_json()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5cee6",
   "metadata": {},
   "source": [
    "## 2. Sport Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25c215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import logging\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.remote.webelement import WebElement\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==============================================================================\n",
    "# KONFIGURASI LOGGING\n",
    "# ==============================================================================\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "class OlympicScraper:\n",
    "    \"\"\"\n",
    "    Scraper komprehensif untuk mengambil data hasil pertandingan Olimpiade Paris 2024,\n",
    "    termasuk event tim, ganda, dan individual, dengan mekanisme checkpoint.\n",
    "    \"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://www.olympics.com/en/olympic-games/paris-2024/results/\"\n",
    "    OUTPUT_FILENAME = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw_sports.json\")\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Menginisialisasi scraper, me-load checkpoint, dan setup WebDriver.\"\"\"\n",
    "        self.scraped_data = self._load_checkpoint()\n",
    "        self.driver = self._setup_driver()\n",
    "        logging.info(\"OlympicScraper berhasil diinisialisasi.\")\n",
    "\n",
    "    def _load_checkpoint(self) -> list:\n",
    "        \"\"\"Memuat data dari file checkpoint JSON jika ada.\"\"\"\n",
    "        if os.path.exists(self.OUTPUT_FILENAME):\n",
    "            try:\n",
    "                with open(self.OUTPUT_FILENAME, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    logging.info(f\"Checkpoint ditemukan. {len(data)} data berhasil dimuat.\")\n",
    "                    return data\n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                logging.error(f\"Gagal memuat checkpoint: {e}. Memulai dari awal.\")\n",
    "                return []\n",
    "        logging.info(\"Tidak ada checkpoint ditemukan. Memulai sesi scraping baru.\")\n",
    "        return []\n",
    "\n",
    "    def _update_checkpoint(self):\n",
    "        \"\"\"Menyimpan/memperbarui data ke file JSON.\"\"\"\n",
    "        try:\n",
    "            with open(self.OUTPUT_FILENAME, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.scraped_data, f, ensure_ascii=False, indent=4)\n",
    "        except IOError as e:\n",
    "            logging.error(f\"Gagal menyimpan checkpoint: {e}\")\n",
    "\n",
    "    def _setup_driver(self) -> webdriver.Chrome:\n",
    "        \"\"\"Mengkonfigurasi instance Selenium WebDriver.\"\"\"\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--log-level=3')\n",
    "        # options.add_argument('--headless') # Aktifkan untuk menjalankan di background\n",
    "        \n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.set_window_size(760, 800)\n",
    "        logging.info(\"WebDriver berhasil di-setup.\")\n",
    "        return driver\n",
    "    \n",
    "    def _handle_cookie_popup(self):\n",
    "        \"\"\"handle pop-up cookie jika muncul.\"\"\"\n",
    "        try:\n",
    "            cookie_accept_button = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n",
    "            )\n",
    "            cookie_accept_button.click()\n",
    "            tqdm.write(\"[*] Pop-up cookie ditemukan dan diterima.\")\n",
    "            time.sleep(1)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    @staticmethod\n",
    "    def slugify(text: str) -> str:\n",
    "        \"\"\"Mengubah teks menjadi format 'slug'\"\"\"\n",
    "        text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"\\'s\\b\", '', text)\n",
    "        text = re.sub(r\"s\\'\\b\", '', text)\n",
    "        text = text.replace('&', ' and ')\n",
    "        text = text.replace('+', ' plus ')\n",
    "        text = re.sub(r'[()]', '', text)\n",
    "        text = re.sub(r'[\\s.,_]+', '-', text)\n",
    "        text = re.sub(r'[^a-z0-9-]', '', text)\n",
    "        text = re.sub(r'-+', '-', text)\n",
    "        text = text.strip('-')\n",
    "        return text\n",
    "\n",
    "    def get_all_sports(self) -> list[dict]:\n",
    "        \"\"\"Mengambil daftar semua cabang olahraga dari halaman utama.\"\"\"\n",
    "        self.driver.get(self.BASE_URL)\n",
    "        self._handle_cookie_popup()\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(self.driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'section[data-cy=\"disciplines-list\"]'))\n",
    "            )\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Gagal memuat daftar olahraga utama: {e}\")\n",
    "            return []\n",
    "\n",
    "        soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "        sport_links = []\n",
    "        \n",
    "        discipline_section = soup.find('section', attrs={'data-cy': 'disciplines-list'})\n",
    "        if not discipline_section:\n",
    "            logging.warning(\"Section 'disciplines-list' tidak ditemukan.\")\n",
    "            return []\n",
    "\n",
    "        links = discipline_section.find_all('a', attrs={'data-cy': 'disciplines-item'})\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            name_element = link.find('p')\n",
    "            if href and name_element:\n",
    "                full_url = f\"https://www.olympics.com{href}\"\n",
    "                sport_links.append({\"name\": name_element.text.strip(), \"url\": full_url})\n",
    "        \n",
    "        logging.info(f\"Ditemukan {len(sport_links)} total cabang olahraga di situs.\")\n",
    "        return sport_links\n",
    "\n",
    "    def get_events_for_sport(self, sport_url: str) -> list[dict]:\n",
    "        \"\"\"Mengambil semua event untuk satu cabang olahraga.\"\"\"\n",
    "        self.driver.get(sport_url)\n",
    "        events_data = []\n",
    "        \n",
    "        dropdown_selector = 'button[data-cy=\"event-select\"]'\n",
    "        button_selector = 'div[data-cy=\"inline-wizard-events\"] button[data-cy=\"event-button\"]'\n",
    "\n",
    "        try:\n",
    "            dropdown_button = WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR, dropdown_selector)))\n",
    "            self.driver.execute_script(\"arguments[0].click();\", dropdown_button)\n",
    "            time.sleep(1)\n",
    "\n",
    "            WebDriverWait(self.driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, button_selector)))\n",
    "            event_buttons = self.driver.find_elements(By.CSS_SELECTOR, button_selector)\n",
    "            \n",
    "            sport_slug = sport_url.rstrip('/').split('/')[-1]\n",
    "            base_event_url = f\"{self.BASE_URL.replace('/results/', f'/results/{sport_slug}/')}\"\n",
    "\n",
    "            for btn in event_buttons:\n",
    "                event_name = btn.find_element(By.TAG_NAME, 'p').text.strip()\n",
    "                event_slug = self.slugify(event_name)\n",
    "                events_data.append({\"name\": event_name, \"url\": f\"{base_event_url}{event_slug}\"})\n",
    "        \n",
    "        except Exception:\n",
    "            page_title = self.driver.title\n",
    "            event_name = page_title.split('-')[1].strip() if '-' in page_title else \"Main Event\"\n",
    "            events_data.append({\"name\": event_name, \"url\": sport_url})\n",
    "            \n",
    "        return events_data\n",
    "\n",
    "    def _scrape_event_page(self, sport: dict, event: dict) -> list[dict]:\n",
    "        \"\"\"Melakukan scraping pada satu halaman event dan mengembalikan hasilnya.\"\"\"\n",
    "        self.driver.get(event['url'])\n",
    "        event_results = []\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'div[data-cy=\"table-content\"]'))\n",
    "            )\n",
    "            time.sleep(3)\n",
    "        except Exception:\n",
    "            tqdm.write(f\"  [!] Gagal memuat konten tabel untuk event: {event['name']}\")\n",
    "            return event_results\n",
    "\n",
    "        result_rows = self.driver.find_elements(By.CSS_SELECTOR, 'div[data-row-id]')\n",
    "        \n",
    "        for row in result_rows:\n",
    "            row_id = row.get_attribute('data-row-id')\n",
    "            row_cy_type = row.get_attribute('data-cy')\n",
    "            \n",
    "            if not row_id or not row_cy_type: continue\n",
    "\n",
    "            row_number = row_id.split('-')[-1]\n",
    "            \n",
    "            rank = self._get_element_text(row, f'div[data-cy=\"medal-row-{row_number}\"] span[data-cy=\"ocs-text-module\"]')\n",
    "            \n",
    "            name_to_use = \"N/A\"\n",
    "            members_list = []\n",
    "            event_type = \"N/A\"\n",
    "\n",
    "            if row_cy_type == 'team-result-row':\n",
    "                event_type = \"Team\"\n",
    "                name_to_use = self._get_element_text(row, f'div[data-cy=\"country-name-row-{row_number}\"] span.iAyztF')\n",
    "                members_list = self._scrape_team_members(row, row_number, name_to_use)\n",
    "            \n",
    "            elif row_cy_type == 'doubles-result-row':\n",
    "                event_type = \"Team\"\n",
    "                name_to_use = self._get_element_text(row, f'div[data-cy=\"flag-row-{row_number}\"] span')\n",
    "                members_list = self._scrape_doubles_event(row)\n",
    "\n",
    "            elif row_cy_type == 'single-athlete-result-row':\n",
    "                event_type = \"Individual\"\n",
    "                name_to_use, members_list = self._scrape_individual_athlete(row)\n",
    "            \n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "            event_results.append({\n",
    "                \"sport\": sport['name'],\n",
    "                \"event\": event['name'],\n",
    "                \"event_url\": event['url'],\n",
    "                \"event_type\": event_type,\n",
    "                \"rank\": rank,\n",
    "                \"team_or_athlete_name\": name_to_use,\n",
    "                \"members\": members_list\n",
    "            })\n",
    "            time.sleep(0.2)\n",
    "        return event_results\n",
    "\n",
    "    def _get_element_text(self, parent: WebElement, selector: str, default: str = \"N/A\") -> str:\n",
    "        \"\"\"Helper untuk mendapatkan teks dari elemen dengan aman.\"\"\"\n",
    "        try:\n",
    "            return parent.find_element(By.CSS_SELECTOR, selector).text.strip()\n",
    "        except:\n",
    "            return default\n",
    "\n",
    "    def _scrape_team_members(self, row_element: WebElement, row_number: str, country_name: str) -> list[dict]:\n",
    "        \"\"\"Scrape anggota tim dari satu baris hasil (event tim besar).\"\"\"\n",
    "        members_list = []\n",
    "        arrow_selector = f'span[data-cy=\"arrow-row-{row_number}\"]'\n",
    "        try:\n",
    "            arrow_element = row_element.find_element(By.CSS_SELECTOR, arrow_selector)\n",
    "            self.driver.execute_script(\"arguments[0].click();\", arrow_element)\n",
    "            \n",
    "            team_container_selector = f'div[data-cy=\"team-members-row-{row_number}\"].open a[data-cy=\"team-member\"]'\n",
    "            WebDriverWait(self.driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, team_container_selector)))\n",
    "            \n",
    "            member_elements = self.driver.find_elements(By.CSS_SELECTOR, team_container_selector)\n",
    "            for member in member_elements:\n",
    "                name = self._get_element_text(member, 'span')\n",
    "                profile_url = member.get_attribute('href')\n",
    "                members_list.append({\"name\": name, \"profile_url\": profile_url})\n",
    "            \n",
    "            self.driver.execute_script(\"arguments[0].click();\", arrow_element)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  [!] Gagal mengambil anggota tim untuk {country_name}: {e}\")\n",
    "        return members_list\n",
    "\n",
    "    def _scrape_doubles_event(self, row_element: WebElement) -> list[dict]:\n",
    "        \"\"\"Scrape data untuk event ganda (duet).\"\"\"\n",
    "        members_list = []\n",
    "        try:\n",
    "            athlete_elements = row_element.find_elements(By.CSS_SELECTOR, 'div[data-cy=\"athlete-image-name\"]')\n",
    "            for athlete_element in athlete_elements:\n",
    "                name = self._get_element_text(athlete_element, 'h3[data-cy=\"athlete-name\"]')\n",
    "                profile_url = athlete_element.find_element(By.CSS_SELECTOR, 'a[data-cy=\"link\"]').get_attribute('href')\n",
    "                members_list.append({\"name\": name, \"profile_url\": profile_url})\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  [!] Gagal mengambil data pemain ganda: {e}\")\n",
    "        return members_list\n",
    "\n",
    "    def _scrape_individual_athlete(self, row_element: WebElement) -> tuple[str, list]:\n",
    "        \"\"\"Scrape data atlet individual.\"\"\"\n",
    "        name = \"N/A\"\n",
    "        members_list = []\n",
    "        try:\n",
    "            name = self._get_element_text(row_element, 'h3[data-cy=\"athlete-name\"]')\n",
    "            profile_url = row_element.find_element(By.CSS_SELECTOR, 'a[data-cy=\"link\"]').get_attribute('href')\n",
    "            members_list.append({\"name\": name, \"profile_url\": profile_url})\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"  [!] Gagal mengambil data pemain individual: {e}\")\n",
    "        return name, members_list\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Menjalankan keseluruhan proses scraping dari awal hingga akhir.\"\"\"\n",
    "        try:\n",
    "            sports_to_scrape = self.get_all_sports()\n",
    "            if not sports_to_scrape:\n",
    "                logging.error(\"Tidak ada cabang olahraga yang ditemukan. Proses dihentikan.\")\n",
    "                return\n",
    "\n",
    "            # add checkpoint\n",
    "            scraped_event_urls = {item['event_url'] for item in self.scraped_data}\n",
    "            \n",
    "            sport_pbar = tqdm(sports_to_scrape, desc=\"Total Progress\", unit=\"sport\")\n",
    "            \n",
    "            for sport in sport_pbar:\n",
    "                sport_pbar.set_description(f\"Processing {sport['name']}\")\n",
    "                \n",
    "                events = self.get_events_for_sport(sport['url'])\n",
    "                if not events:\n",
    "                    tqdm.write(f\"  [-] Tidak ada event untuk {sport['name']}.\")\n",
    "                    continue\n",
    "                \n",
    "                new_data_for_this_sport = []\n",
    "                event_pbar = tqdm(events, desc=f\"  -> Events\", unit=\"event\", leave=False)\n",
    "                for event in event_pbar:\n",
    "                    \n",
    "                    # checkpoint\n",
    "                    if event['url'] in scraped_event_urls:\n",
    "                        continue \n",
    "\n",
    "                    event_pbar.set_description(f\"  -> Scraping {event['name'][:30]}...\")\n",
    "                    results = self._scrape_event_page(sport, event)\n",
    "                    new_data_for_this_sport.extend(results)\n",
    "\n",
    "                # simpan checkpoint (safety)\n",
    "                if new_data_for_this_sport:\n",
    "                    self.scraped_data.extend(new_data_for_this_sport)\n",
    "                    self._update_checkpoint()\n",
    "                    tqdm.write(f\"[✔] Checkpoint untuk {sport['name']} berhasil diperbarui.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Terjadi kesalahan fatal: {e}\", exc_info=True)\n",
    "        finally:\n",
    "            self.driver.quit()\n",
    "            logging.info(\"Browser telah ditutup. Proses selesai.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = OlympicScraper()\n",
    "    scraper.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ef9037",
   "metadata": {},
   "source": [
    "## 3. Athlete Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260bd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "class AthleteScraperPipeline:\n",
    "    \n",
    "    SPORTS_DATA_INPUT_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw_sports.json\")\n",
    "    ATHLETES_DATA_OUTPUT_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw_athletes.json\")\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Inisialisasi pipeline dengan memuat data yang dibutuhkan dan setup scraper.\n",
    "        \"\"\"\n",
    "        self.athlete_urls_to_scrape = self._get_unique_athlete_urls()\n",
    "        self.scraper = AthleteProfileScraper()\n",
    "\n",
    "    def _get_unique_athlete_urls(self) -> list[str]:\n",
    "        \"\"\"\n",
    "        Membaca file input, mengekstrak URL profil unik, dan mengurutkannya.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.SPORTS_DATA_INPUT_FILE):\n",
    "            logging.error(f\"File input '{self.SPORTS_DATA_INPUT_FILE}' tidak ditemukan. Jalankan sport_scraper.py terlebih dahulu.\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            with open(self.SPORTS_DATA_INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "                sports_data = json.load(f)\n",
    "            \n",
    "            all_urls = set()\n",
    "            for item in sports_data:\n",
    "                if \"members\" in item and item[\"members\"]:\n",
    "                    for member in item[\"members\"]:\n",
    "                        if \"profile_url\" in member and member[\"profile_url\"]:\n",
    "                            all_urls.add(member[\"profile_url\"])\n",
    "            \n",
    "            logging.info(f\"Ditemukan {len(all_urls)} URL atlet unik dari {len(sports_data)} entri data olahraga.\")\n",
    "            return sorted(list(all_urls))\n",
    "\n",
    "        except (json.JSONDecodeError, IOError) as e:\n",
    "            logging.error(f\"Gagal membaca atau mem-parsing file '{self.SPORTS_DATA_INPUT_FILE}': {e}\")\n",
    "            return []\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        Menjalankan keseluruhan proses scraping profil atlet.\n",
    "        \"\"\"\n",
    "        if not self.athlete_urls_to_scrape:\n",
    "            logging.warning(\"Tidak ada URL atlet untuk di-scrape. Proses dihentikan.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            self.scraper.scrape_all(self.athlete_urls_to_scrape, self.ATHLETES_DATA_OUTPUT_FILE)\n",
    "        except Exception as e:\n",
    "            logging.critical(f\"Terjadi kesalahan fatal selama proses pipeline: {e}\", exc_info=True)\n",
    "        finally:\n",
    "            self.scraper.close()\n",
    "\n",
    "class AthleteProfileScraper:\n",
    "    \"\"\"\n",
    "    Scraper yang didedikasikan untuk mengambil data dari halaman profil atlet.\n",
    "    \"\"\"\n",
    "    CHECKPOINT_INTERVAL = 50 # Interval checkpoint (50 data)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.driver = self._setup_driver()\n",
    "        self.scraped_data = []\n",
    "\n",
    "    def _setup_driver(self) -> webdriver.Chrome:\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--log-level=3')\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        driver.set_window_size(1024, 768)\n",
    "        return driver\n",
    "\n",
    "    def _load_checkpoint(self, filename: str) -> list:\n",
    "        if os.path.exists(filename):\n",
    "            try:\n",
    "                with open(filename, 'r', encoding='utf-8') as f:\n",
    "                    data = json.load(f)\n",
    "                    logging.info(f\"Checkpoint atlet ditemukan. {len(data)} data berhasil dimuat.\")\n",
    "                    return data\n",
    "            except (json.JSONDecodeError, IOError): return []\n",
    "        return []\n",
    "\n",
    "    def _update_checkpoint(self, filename: str):\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.scraped_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    def _get_profile_section(self, url: str) -> Tag | None:\n",
    "        self.driver.get(url)\n",
    "        try:\n",
    "            WebDriverWait(self.driver, 20).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, 'section[data-cy=\"athlete-profile\"]'))\n",
    "            )\n",
    "            time.sleep(2)\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            return soup.find('section', attrs={'data-cy': 'athlete-profile'})\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _parse_medals(self, profile_section: Tag) -> dict:\n",
    "        medals = {'gold': 0, 'silver': 0, 'bronze': 0}\n",
    "        medal_elements = profile_section.select('div[data-cy=\"medal-module\"]')\n",
    "        for medal in medal_elements:\n",
    "            try:\n",
    "                count = int(medal.select_one('span[data-cy=\"medal-main\"]').get_text(strip=True))\n",
    "                medal_type_char = medal.select_one('span[data-cy=\"medal-additional\"]').get_text(strip=True)\n",
    "                if medal_type_char == 'S': medals['silver'] = count\n",
    "                elif medal_type_char == 'B': medals['bronze'] = count\n",
    "                elif medal_type_char == 'G': medals['gold'] = count\n",
    "            except (ValueError, AttributeError): continue\n",
    "        return medals\n",
    "\n",
    "    def scrape_all(self, urls: list[str], output_filename: str):\n",
    "        self.scraped_data = self._load_checkpoint(output_filename)\n",
    "        scraped_urls = {item['url'] for item in self.scraped_data}\n",
    "        \n",
    "        # Filter URL yang belum di-scrape (kebutuhan checkpoint)\n",
    "        urls_to_process = [url for url in urls if url not in scraped_urls]\n",
    "        \n",
    "        if not urls_to_process:\n",
    "            logging.info(\"Semua data atlet sudah lengkap sesuai checkpoint. Tidak ada yang perlu di-scrape.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Akan memproses {len(urls_to_process)} atlet baru.\")\n",
    "        \n",
    "        new_entries_since_last_checkpoint = 0\n",
    "        url_pbar = tqdm(urls_to_process, desc=\"Scraping Athlete Profiles\", unit=\"athlete\")\n",
    "\n",
    "        for url in url_pbar:\n",
    "            profile_section = self._get_profile_section(url)\n",
    "            if not profile_section:\n",
    "                tqdm.write(f\"[!] Gagal memuat profil untuk: {url}\")\n",
    "                continue\n",
    "            \n",
    "            def get_text(selector: str, default: str = \"N/A\") -> str:\n",
    "                element = profile_section.select_one(selector)\n",
    "                return element.get_text(strip=True) if element else default\n",
    "\n",
    "            data = {\n",
    "                \"url\": url,\n",
    "                \"name\": get_text('div[data-cy=\"display-name\"] h1'),\n",
    "                \"country\": get_text('div[data-cy=\"nocs\"] span'),\n",
    "                \"discipline\": get_text('div[data-cy=\"disciplines\"] span'),\n",
    "                \"game_participations\": get_text('span[data-cy=\"games-participations\"]'),\n",
    "                \"first_olympic_games\": get_text('span[data-cy=\"first-olympic-game\"]'),\n",
    "                \"year_of_birth\": get_text('span[data-cy=\"year-of-birth\"]'),\n",
    "                \"olympic_medals\": self._parse_medals(profile_section)\n",
    "            }\n",
    "            \n",
    "            url_pbar.set_description(f\"Processing {data['name']}\")\n",
    "            self.scraped_data.append(data)\n",
    "            new_entries_since_last_checkpoint += 1\n",
    "\n",
    "            # Save setiap 50 data berhasil di-scrape (checkpoint)\n",
    "            if new_entries_since_last_checkpoint >= self.CHECKPOINT_INTERVAL:\n",
    "                self._update_checkpoint(output_filename)\n",
    "                tqdm.write(f\"[✔] Checkpoint disimpan ({len(self.scraped_data)} total atlet).\")\n",
    "                new_entries_since_last_checkpoint = 0\n",
    "            \n",
    "            time.sleep(1)\n",
    "\n",
    "        if new_entries_since_last_checkpoint > 0:\n",
    "            self._update_checkpoint(output_filename)\n",
    "            logging.info(f\"Penyimpanan final dilakukan. Total data atlet: {len(self.scraped_data)}\")\n",
    "\n",
    "        logging.info(\"Proses scraping semua atlet selesai.\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = AthleteScraperPipeline()\n",
    "    pipeline.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c008ca31",
   "metadata": {},
   "source": [
    "## 4. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a8cdc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19:52:15 [INFO] Membaca file input: c:\\Izhar\\Lab Basdat\\Seleksi-2025-Tugas-1\\Data Scraping\\data\\raw_sports.json dan c:\\Izhar\\Lab Basdat\\Seleksi-2025-Tugas-1\\Data Scraping\\data\\raw_athletes.json\n",
      "19:52:15 [INFO] Data berhasil dimuat. Total 13978 partisipasi dan 5028 profil atlet.\n",
      "19:52:15 [INFO] Memulai proses cleaning dan transformasi...\n",
      "19:52:16 [INFO] Mengubah nama 'n/a' dan negara 'team' menjadi 'unknown' di data sports.\n",
      "19:52:16 [INFO] Menstandarkan nama negara (hong kong, n/a, virgin islands).\n",
      "19:52:16 [INFO] Mengisi nilai null untuk partisipasi (1) dan tahun lahir (0).\n",
      "19:52:16 [INFO] Membuat tabel entitas...\n",
      "19:52:16 [INFO] Membuat Tabel Partisipasi...\n",
      "19:52:16 [WARNING] Membuang 1 baris partisipasi duplikat.\n",
      "19:52:16 [INFO] Tabel Partisipasi berhasil dibuat dengan 7331 entri unik.\n",
      "19:52:16 [INFO] Menyimpan semua tabel ke file CSV...\n",
      "19:52:16 [INFO] Semua file telah disimpan di dalam folder 'c:\\Izhar\\Lab Basdat\\Seleksi-2025-Tugas-1\\Data Scraping\\data\\cleaned_data'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import logging\n",
    "import os\n",
    "\n",
    "# ==============================================================================\n",
    "# KONFIGURASI\n",
    "# ==============================================================================\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s [%(levelname)s] %(message)s', datefmt='%H:%M:%S')\n",
    "\n",
    "# --- Konfigurasi File ---\n",
    "SPORTS_INPUT_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw_sports.json\")\n",
    "ATHLETES_INPUT_FILE = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"raw_athletes.json\")\n",
    "OUTPUT_DIR = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"cleaned_data\")\n",
    "\n",
    "# ==============================================================================\n",
    "# FUNGSI TRANSFORMASI\n",
    "# ==============================================================================\n",
    "\n",
    "def transform_rank(rank: str) -> str:\n",
    "    \"\"\"\n",
    "    Mengubah format rank sesuai aturan:\n",
    "    - G/S/B -> emas/perak/perunggu\n",
    "    - Angka (e.g., '5', '=9') -> #angka (e.g., '#5', '#9')\n",
    "    \"\"\"\n",
    "    if not isinstance(rank, str):\n",
    "        return None\n",
    "    \n",
    "    rank = rank.lower().strip()\n",
    "    \n",
    "    if rank == 'g': return 'emas'\n",
    "    if rank == 's': return 'perak'\n",
    "    if rank == 'b': return 'perunggu'\n",
    "    \n",
    "    rank_numeric = ''.join(filter(str.isdigit, rank))\n",
    "    if rank_numeric:\n",
    "        return f\"#{rank_numeric}\"\n",
    "        \n",
    "    return None\n",
    "\n",
    "# ==============================================================================\n",
    "# PROSES UTAMA\n",
    "# ==============================================================================\n",
    "\n",
    "def preprocess_data():\n",
    "    \"\"\"\n",
    "    Fungsi utama untuk memuat, membersihkan, mentransformasi,\n",
    "    dan menyimpan data dari semua sumber ke dalam file CSV yang ternormalisasi.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Membaca file input: {SPORTS_INPUT_FILE} dan {ATHLETES_INPUT_FILE}\")\n",
    "        df_sports_raw = pd.read_json(SPORTS_INPUT_FILE)\n",
    "        df_sports = pd.json_normalize(df_sports_raw.to_dict('records'), 'members', \n",
    "                                          ['sport', 'event', 'event_type', 'rank', 'team_or_athlete_name'])\n",
    "        \n",
    "        df_athletes_raw = pd.read_json(ATHLETES_INPUT_FILE)\n",
    "        logging.info(f\"Data berhasil dimuat. Total {len(df_sports)} partisipasi dan {len(df_athletes_raw)} profil atlet.\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error: File tidak ditemukan -> {e}. Pastikan kedua file JSON ada.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saat memuat data: {e}\")\n",
    "        return\n",
    "\n",
    "    #  Cleaning dan Transformasi Awal\n",
    "    logging.info(\"Memulai proses cleaning dan transformasi...\")\n",
    "    \n",
    "    # --- Proses df_sports ---\n",
    "    df_sports = df_sports.drop(columns=['profile_url'])\n",
    "    for col in ['sport', 'event', 'event_type', 'team_or_athlete_name', 'name']:\n",
    "        df_sports[col] = df_sports[col].str.lower()\n",
    "    df_sports['medali'] = df_sports['rank'].apply(transform_rank)\n",
    "    df_sports = df_sports.drop(columns=['rank'])\n",
    "\n",
    "    df_sports.loc[df_sports['name'] == 'n/a', 'name'] = 'unknown'\n",
    "    df_sports.loc[df_sports['team_or_athlete_name'] == 'team', 'team_or_athlete_name'] = 'unknown'\n",
    "    logging.info(\"Mengubah nama 'n/a' dan negara 'team' menjadi 'unknown' di data sports.\")\n",
    "    \n",
    "    # --- Proses df_athletes_raw ---\n",
    "    df_athletes_raw = df_athletes_raw.drop(columns=['url', 'olympic_medals'])\n",
    "    for col in ['name', 'country', 'discipline', 'first_olympic_games']:\n",
    "        df_athletes_raw[col] = df_athletes_raw[col].str.lower()\n",
    "    \n",
    "    df_athletes_raw.loc[df_athletes_raw['name'] == 'n/a', 'name'] = 'unknown'\n",
    "    df_athletes_raw.loc[df_athletes_raw['country'] == 'team', 'country'] = 'unknown'\n",
    "    df_athletes_raw['country'] = df_athletes_raw['country'].replace({\n",
    "        'hong kong, china': 'hong kong',\n",
    "        'n/a': 'unknown',\n",
    "        'virgin islands, british': 'unknown'\n",
    "    })\n",
    "    logging.info(\"Menstandarkan nama negara (hong kong, n/a, virgin islands).\")\n",
    "    \n",
    "    original_rows = len(df_athletes_raw)\n",
    "    df_athletes_raw.dropna(subset=['country'], inplace=True)\n",
    "    if original_rows > len(df_athletes_raw):\n",
    "        logging.warning(f\"Membuang {original_rows - len(df_athletes_raw)} baris dari data atlet karena tidak memiliki data negara.\")\n",
    "\n",
    "    for col in ['game_participations', 'year_of_birth']:\n",
    "        df_athletes_raw[col] = pd.to_numeric(df_athletes_raw[col], errors='coerce')\n",
    "    \n",
    "    df_athletes_raw['game_participations'] = df_athletes_raw['game_participations'].fillna(1)\n",
    "    df_athletes_raw['year_of_birth'] = df_athletes_raw['year_of_birth'].fillna(0)\n",
    "    \n",
    "    for col in ['game_participations', 'year_of_birth']:\n",
    "        df_athletes_raw[col] = df_athletes_raw[col].astype(int)\n",
    "    logging.info(\"Mengisi nilai null untuk partisipasi (1) dan tahun lahir (0).\")\n",
    "\n",
    "    # --- Membuat Tabel Entitas ---\n",
    "    \n",
    "    logging.info(\"Membuat tabel entitas...\")\n",
    "    all_countries = df_athletes_raw['country'].dropna().unique()\n",
    "    df_negara = pd.DataFrame(all_countries, columns=['nama_negara'])\n",
    "    df_negara['benua'] = None\n",
    "\n",
    "    df_pertandingan = df_sports[['sport', 'event', 'event_type']].drop_duplicates().reset_index(drop=True)\n",
    "    df_pertandingan = df_pertandingan.rename(columns={'sport': 'nama_olahraga', 'event': 'nama_pertandingan', 'event_type': 'jenis_pertandingan'})\n",
    "    df_pertandingan.insert(0, 'ID', df_pertandingan.index + 1)\n",
    "    \n",
    "    df_atlet = df_athletes_raw.rename(columns={'name': 'nama', 'country': 'nama_negara', 'discipline': 'cabang_olahraga', 'game_participations': 'jumlah_partisipasi_olimpiade', 'first_olympic_games': 'olimpiade_pertama', 'year_of_birth': 'tahun_lahir'})\n",
    "    df_atlet.insert(0, 'ID', df_atlet.index + 1)\n",
    "\n",
    "    # --- Membuat Tabel Relasi (Partisipasi) ---\n",
    "    logging.info(\"Membuat Tabel Partisipasi...\")\n",
    "    \n",
    "    df_partisipasi = pd.merge(df_sports, df_pertandingan, \n",
    "                              left_on=['sport', 'event', 'event_type'], \n",
    "                              right_on=['nama_olahraga', 'nama_pertandingan', 'jenis_pertandingan'])\n",
    "                              \n",
    "    df_partisipasi = pd.merge(df_partisipasi, df_atlet, \n",
    "                              left_on='name',\n",
    "                              right_on='nama')\n",
    "                              \n",
    "    df_partisipasi = df_partisipasi[['ID_y', 'ID_x', 'medali']].rename(columns={'ID_y': 'ID_atlet', 'ID_x': 'ID_pertandingan'})\n",
    "    \n",
    "    original_count = len(df_partisipasi)\n",
    "    df_partisipasi.drop_duplicates(subset=['ID_atlet', 'ID_pertandingan'], inplace=True)\n",
    "    if len(df_partisipasi) < original_count:\n",
    "        logging.warning(f\"Membuang {original_count - len(df_partisipasi)} baris partisipasi duplikat.\")\n",
    "    \n",
    "    logging.info(f\"Tabel Partisipasi berhasil dibuat dengan {len(df_partisipasi)} entri unik.\")\n",
    "\n",
    "    # --- Menyimpan Semua Tabel ke File CSV ---\n",
    "    logging.info(\"Menyimpan semua tabel ke file CSV...\")\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    df_negara[['nama_negara', 'benua']].to_csv(f\"{OUTPUT_DIR}/negara.csv\", index=False)\n",
    "    df_pertandingan[['ID', 'nama_olahraga', 'nama_pertandingan', 'jenis_pertandingan']].to_csv(f\"{OUTPUT_DIR}/pertandingan.csv\", index=False)\n",
    "    df_atlet[['ID', 'nama_negara', 'nama', 'cabang_olahraga', 'jumlah_partisipasi_olimpiade', 'olimpiade_pertama', 'tahun_lahir']].to_csv(f\"{OUTPUT_DIR}/atlet.csv\", index=False)\n",
    "    df_partisipasi.to_csv(f\"{OUTPUT_DIR}/partisipasi.csv\", index=False)\n",
    "    \n",
    "    logging.info(f\"Semua file telah disimpan di dalam folder '{OUTPUT_DIR}'.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_data()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
